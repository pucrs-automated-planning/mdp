{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDPs in pymdptoolbox - Class Assignment\n",
    "\n",
    "In this practical exercise, we will look at how MDP planning is implemented in a mathematical toolkit, and track the calculation of the rewards for each state via Value Iteration. The following code sets up an MDP environment (the basic case shown in class, shown in the Figure below) and computes the policy for the given MDP using the Value Iteration.\n",
    "\n",
    "<img align=\"center\" src=\"mdp_simple.png\"/>\n",
    "\n",
    "Then we provide a set of questions for you to implement and answer. This assignment is not graded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E' 'E' 'E' 'T']\n",
      " ['N' 'O' 'W' 'T']\n",
      " ['N' 'W' 'W' 'S']]\n"
     ]
    }
   ],
   "source": [
    "# The line below is to be used if you have pymdptoolbox installed with setuptools\n",
    "# import mdptoolbox.example\n",
    "# Whereas the line below obviate the need to install that\n",
    "import sys\n",
    "sys.path.insert(0, 'pymdptoolbox/src')\n",
    "import mdptoolbox.example\n",
    "\n",
    "import numpy as _np\n",
    "from gen_scenario import *\n",
    "\n",
    "\"\"\"\n",
    "(Y,X)\n",
    "| 00 01 02 ... 0X-1       'N' = North\n",
    "| 10  .         .         'S' = South\n",
    "| 20    .       .         'W' = West\n",
    "| .       .     .         'E' = East\n",
    "| .         .   .         'T' = Terminal\n",
    "| .           . .         'O' = Obstacle\n",
    "| Y-1,0 . . .   Y-1X-1\n",
    "\"\"\" \n",
    "\n",
    "shape = [3,4]\n",
    "rewards = [[0,3,100],[1,3,-100]]\n",
    "obstacles = [[1,1]]\n",
    "terminals = [[0,3],[1,3]]\n",
    "P, R = mdp_grid(shape=shape, terminals=terminals, r=-3, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIterationGS(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire\n",
    "1. Study the code of the cell above and answer the following questions.\n",
    "\t1. What is the policy generated if we change the discount factor of the grid domain to 0.1?\n",
    "\t2. Use the following line ```vi.verbose = True\t``` before  ```vi.run()```:   \n",
    "\tWhat is the variation for each of the first three iterations with the discount factor of 0.9 and how many iterations does the algorithm take to converge?\n",
    "\t3. How does changes to the discount factor affect the variation of the state values over time?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The scenario below has an interesting structure whereby the positive rewarding terminal state is partially surrounded by negatively-rewarding states. Program this scenario in pymdptoolbox and compute the optimal policy with a discount factor of 0.99.\n",
    "\n",
    "<img align=\"center\" src=\"mdp-odd.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define two new 5 by 5 scenarios with multiple obstacles and an interesting geometry following the guidelines below. Calculate the policy with discount factor 0.99, and then try to explain intuitively the reason for the resulting policies, given the initial parameters. These two scenarios must have the following characteristics:\n",
    "\t1. A scenario with one (or more) terminal states with positive rewards and at least one other state with the same amount of, but negative reward and no terminal states with negative rewards.\n",
    "\t2. A scenario with one terminal state with a negative reward and at least one non-terminal state with a positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
